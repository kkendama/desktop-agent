#!/usr/bin/env python3
"""
Quick test script to verify basic functionality.
"""

import sys
import asyncio
from pathlib import Path

# Add current directory to path
sys.path.insert(0, str(Path(__file__).parent))

from core.config import ConfigManager
from core.llm.manager import LLMManager
from core.llm.base import LLMMessage


async def test_basic_functionality():
    """Test basic functionality without requiring LLM server."""
    
    print("üîß Testing Desktop Agent Basic Functionality")
    print("=" * 50)
    
    # Test 1: Configuration Loading
    print("\n1Ô∏è‚É£  Testing configuration loading...")
    try:
        config_manager = ConfigManager()
        config_valid = config_manager.validate_configs()
        print(f"   ‚úÖ Configuration validation: {config_valid}")
        
        # Test specific config loading
        system_config = config_manager.get_system_config()
        llm_config = config_manager.get_llm_config()
        print(f"   ‚úÖ LLM Provider: {llm_config.get('provider', 'unknown')}")
        print(f"   ‚úÖ LLM Model: {llm_config.get('model', 'unknown')}")
        
    except Exception as e:
        print(f"   ‚ùå Configuration test failed: {e}")
        return False
    
    # Test 2: Prompt Template Formatting
    print("\n2Ô∏è‚É£  Testing prompt templates...")
    try:
        greeting = config_manager.get_user_greeting()
        print(f"   ‚úÖ User greeting loaded: {len(greeting)} characters")
        
        system_prompt = config_manager.get_system_prompt(
            current_datetime="2024-01-01 12:00:00",
            workspace_path="/test/workspace"
        )
        print(f"   ‚úÖ System prompt formatted: {len(system_prompt)} characters")
        
    except Exception as e:
        print(f"   ‚ùå Prompt template test failed: {e}")
        return False
    
    # Test 3: LLM Manager Initialization (without server)
    print("\n3Ô∏è‚É£  Testing LLM manager...")
    try:
        llm_manager = LLMManager()
        await llm_manager.load_config()
        
        provider_info = llm_manager.get_provider_info()
        print(f"   ‚úÖ Provider info: {provider_info}")
        
        available_providers = llm_manager.list_available_providers()
        print(f"   ‚úÖ Available providers: {available_providers}")
        
    except Exception as e:
        print(f"   ‚ùå LLM manager test failed: {e}")
        return False
    
    # Test 4: Message Objects
    print("\n4Ô∏è‚É£  Testing message objects...")
    try:
        user_msg = LLMMessage(role="user", content="Hello, AI!")
        assistant_msg = LLMMessage(role="assistant", content="Hello, human!")
        
        print(f"   ‚úÖ User message: {user_msg.role} - {user_msg.content}")
        print(f"   ‚úÖ Assistant message: {assistant_msg.role} - {assistant_msg.content}")
        
    except Exception as e:
        print(f"   ‚ùå Message objects test failed: {e}")
        return False
    
    print("\nüéâ All basic tests passed!")
    print("\nüìã Next Steps:")
    print("   1. Start Ollama server: `ollama serve`")
    print("   2. Pull Qwen3 model: `ollama pull qwen3:latest`")
    print("   3. Run Desktop Agent: `python cli/main.py`")
    
    return True


if __name__ == "__main__":
    success = asyncio.run(test_basic_functionality())
    sys.exit(0 if success else 1)